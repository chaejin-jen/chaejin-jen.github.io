{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class 05 선형회귀<sup>Linear Regression</sup>\n",
    "\n",
    "- 주요 참고 문헌 :  Pattern Recognition and Machine Learning, Christopher Bishop, Springer<sup>[Bishop]</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>var waitForPlotly = setInterval( function() {if( typeof(window.Plotly) !== \"undefined\" ){MathJax.Hub.Config({ SVG: { font: \"STIX-Web\" }, displayAlign: \"center\" });MathJax.Hub.Queue([\"setRenderer\", MathJax.Hub, \"SVG\"]);clearInterval(waitForPlotly);}}, 250 );</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# matplotlib style\n",
    "style_name = 'bmh' #bmh\n",
    "mpl.style.use(style_name)\n",
    "style = plt.style.library[style_name]\n",
    "style_colors = [ c['color'] for c in style['axes.prop_cycle'] ]\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth =150)\n",
    "\n",
    "##########################################################\n",
    "# for sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "##########################################################\n",
    "# for plotly\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# https://github.com/plotly/plotly.py/issues/515\n",
    "# The polling here is to ensure that plotly.js has already been loaded before\n",
    "# setting display alignment in order to avoid a race condition.\n",
    "display(HTML(\n",
    "    '<script>'\n",
    "        'var waitForPlotly = setInterval( function() {'\n",
    "            'if( typeof(window.Plotly) !== \"undefined\" ){'\n",
    "                'MathJax.Hub.Config({ SVG: { font: \"STIX-Web\" }, displayAlign: \"center\" });'\n",
    "                'MathJax.Hub.Queue([\"setRenderer\", MathJax.Hub, \"SVG\"]);'\n",
    "                'clearInterval(waitForPlotly);'\n",
    "            '}}, 250 );'\n",
    "    '</script>'\n",
    "))\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전작업\n",
    "\n",
    "- 4주차 만들었던 `minimize()` 함수를 불러 온다.\n",
    "\n",
    "- 이 노트북에서 실행되는 모든 실험은 기본적으로 `scipy.optimize.minimize()`를 이용하지만 주석처리된 부분을 바꿔주면 직접 코딩한 최적화 함수 `minimize()`를 이용해서도 동일한 결과를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try import optimize_ans\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../class-04/notebook')\n",
    "\n",
    "# 함수 임포트\n",
    "try:\n",
    "    print(\"try import optimize_ans\")\n",
    "    from optimize_ans import minimize\n",
    "except ImportError:\n",
    "    print(\"import optimize\")\n",
    "    from optimize import minimize\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 목표\n",
    "\n",
    "- 주어진 데이터를 대표하는 곡선을 찾기 위해 최소제곱법을 구성한다.\n",
    "\n",
    "- 구성된 최소제곱법을 풀어내기 위해 `scipy.optimize.minimize()`와 직접 코딩한 `minimize()`를 적용한다.\n",
    "\n",
    "- 모델의 복잡도가 문제를 변화시키는 양상과 규제화<sup>regularization</sup>의 메커니즘을 이해한다.\n",
    "\n",
    "- 볼록계획<sup>convex programming problem</sup>문제인 선형회귀를 해석적으로 풀기 위한 과정인 정규방정식을 이해한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 데이터 생성\n",
    "\n",
    "- 실험용 데이터를 생성하기 위해곡선 $\\sin(2\\pi x)$에 적당한 노이즈를 더하여 데이터를 생성\n",
    "\n",
    "\n",
    "- $\\boldsymbol{\\mathsf{x}}_{\\text{train}} = (x_1, x_2, \\dots, x_N)^{\\text{T}}$ : \n",
    "[0,1]구간을 등간격으로 잘라 만든 $x$ 값 입력 데이터로 $N=10$으로 데이터 10개를 생성\n",
    "\n",
    "- $\\boldsymbol{\\mathsf{t}}_{\\text{train}} = (t_1, t_2, \\dots, t_N)^{\\text{T}}$ : $\\sin(2\\pi\\boldsymbol{\\mathsf{x}})+\\text{noise}$로 만들어진 출력 데이터\n",
    "\n",
    "- 같은 방식으로 학습데이터<sup>train data</sup>, 테스트 데이터<sup>test data</sup>를 각각 따로 준비\n",
    "\n",
    "- 학습데이터 : 선형 회귀 모델을 학습하는데 사용하는 데이터\n",
    "\n",
    "- 테스트데이터 : 학습이 끝난 선형 회귀 모델을 평가하기 위해 사용하는 데이터\n",
    "\n",
    "- 좀 더 엄격하게는 학습데이터, 검증데이터, 테스트데이터 3개로 구분하고 검증데이트를 모델의 하이퍼 파라미터를 결정하는데 사용하고 최종적으로 튜닝을 마친 모델에 테스트데이터를 사용하여 최종 성능을 평가함\n",
    "\n",
    "- 하이퍼 파라미터 튜닝은 본 수업 범위를 벗어나므로 검증데이터는 생략하였음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기호법\n",
    "\n",
    "- $\\mathcal{D}= (x, t)$ : 주어진 데이터를 의미하며 입력$x$와 타겟 $t$의 쌍으로 이루어 짐\n",
    "\n",
    "- $D$ : 입력 데이터의 차원\n",
    "\n",
    "- $\\mathbf{x}$ : 입력 데이터, 일반적으로 $D$차원, $D \\times 1$ 열벡터, $x$로 쓰면 스칼라\n",
    "\n",
    "- $N$ : 데이터의 개수\n",
    "\n",
    "- $\\mathbf{X}$ : $N$개의 입력 데이터 $\\mathbf{x}$의 집합, 따라서 $N \\times D$ 행렬, 즉 데이터 $\\mathbf{x}$가 행벡터\n",
    "\n",
    "- $t$ : 입력에 대응되는 출력, 스칼라\n",
    "\n",
    "- $\\boldsymbol{\\mathsf{t}}$ : 스칼라 $t$의 집합으로 $N \\times 1$ 열벡터, 스칼라 변수의 집합을 나타내며 벡터변수를 나타내는 볼드 $\\mathbf{t}$와는 구별됨\n",
    "\n",
    "- $\\mathbf{w}$ : 회귀계수, 기저함수를 쓴다면 $M \\times 1$ 열벡터, 쓰지 않으면 $D \\times 1$ 열벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트에 쓸 데이터를 정의한 노트북 파일을 로드한다.\n",
    "%run ./data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)\n",
    "print(t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## 선형모델<sup>linear model</sup>\n",
    "\n",
    "- 준비한 학습데이터를 잘 표현하는 직선을 찾고 싶다면 다음과 같은 직선을 생각해볼 수 있다.\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "- 현재 문제에서 입력 $x$는 스칼라(숫자 하나), 결정해야하는 파라미터 $w_0$, $w_1$ 두개 \n",
    "\n",
    "- 위 식에서 $w_0$뒤에 숫자 1이 곱해져있다고 생각해보면 다음처럼 쓸 수 있다.\n",
    "\n",
    "$$\n",
    "y = w_0 \\times 1 + w_1 \\times x\n",
    "$$\n",
    "\n",
    "- 위 식을 벡터 형식으로 써보면 다음과 같은데\n",
    "\n",
    "$$\n",
    "y = w_0 \\times 1 + w_1 \\times x =  (w_0, w_1) \\cdot \\begin{pmatrix} 1 \\\\ x\\end{pmatrix}= \\mathbf{w}^{\\text{T}} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "- 주어진 문제에서 입력 $x$는 스칼라이지만 이때 $\\mathbf{x}$는 첫번째 요소에 항상 1이 오는 요소 개수 2인 벡터가 된다.\n",
    "\n",
    "- $x$를 입력되는 데이터의 특성이라고 본다면 우리는 1이라는 아무 의미없는 특성을 하나 추가하여 스칼라 데이터를 벡터 데이터로 확장한것으로 볼 수 도 있다.\n",
    "\n",
    "- $w_0=1$, $w_1=-2$로 설정하고 주어진 데이터에 $\\mathbf{w}$를 곱하여 타겟 $\\boldsymbol{\\mathsf{t}}$를 구하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train은 숫자 1개인 입력데이터 10개가 있는 1차원 데이터\n",
    "print(\"원래 x_train\")\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "\n",
    "# 결정해야하는 두개의 w\n",
    "w0 = 1.0\n",
    "w1 = -2.\n",
    "w = np.array([w0, w1])\n",
    "\n",
    "# 첫번째 요소에 1을 추가하고\n",
    "X = np.array([np.ones_like(x_train), x_train]).T\n",
    "print(\"첫 요소에 1을 추가하여 각각을 벡터로 만든 x_train : X\")\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print('\\n')\n",
    "\n",
    "#w와 내적을 하면 결과값 10개가 만들어 진다. w^t X\n",
    "print(\"w와 X의 내적\")\n",
    "print(np.dot(X,w.reshape(-1,1)))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, x*w[1]+w[0])\n",
    "plt.plot(x_train, t_train, 'o', alpha=0.8, label=\"train data\")\n",
    "plt.plot(x_train, np.dot(X,w.reshape(-1,1)), 'o', label=r\"$\\mathbf{w}^T \\mathbf{x}_{train}$\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 보라색 포인트가 계산된 타겟값인데 맞추기를 원하는 붉은색 타겟과는 차이가 많이 나는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 다항식 기저\n",
    "\n",
    "- 위 과정으로 부터 직선으로 데이터를 잘 표현할 수 있을것 같지 않다.\n",
    "\n",
    "- 다음과 같은 곡선을 생각해보자\n",
    "\n",
    "$$\n",
    "y(x, \\mathbf{w}) = w_0 x^0 + w_1 x^1 + w_2 x^2 + \\cdots + w_M x^M = \\sum_{j=0}^{M} w_j x^j\n",
    "$$\n",
    "\n",
    "- $M=1$이면 앞서 이야기한 직선과 동일\n",
    "\n",
    "- 위 다항식은 $x$에 대해서 비선형, $w$에 대해서 선형($w$에 대해서 제곱이상의 항이 없음) 따라서 선형모델이라 한다.\n",
    "\n",
    "- $w$에 대해 선형이란 의미는 `linearity.ipynb`을 보면 더 확실히 알 수 있다.\n",
    "\n",
    "- 위 다항식이 $\\sin(2\\pi x)$와 비슷하기를 원하는 것이다.\n",
    "\n",
    "- 다르게 말하면 \"데이터를 발생시킨 곡선이 위 다항식과 비슷하지 않았을까?\" 하고 가정하는 것\n",
    "\n",
    "- 따라서 가끔 위 다항식 $y(x, \\mathbf{w})$을 가설<sup>hypothesis</sup>로 부르기도 함. 즉 우리의 가설이 잘 맞기를 바라는 상황이다.\n",
    "\n",
    "- 머신러닝의 특성<sup>feature</sup>관점에서 보면 주어진 데이터는 $x$인데 여기에 $x^0$, $x^2$, ... , $x^M$ 이라는 새로운 숫자들을 추가한 것이므로 회귀에 필요한 특성을 더 추가 해준것으로 볼 수 도 있다.\n",
    "\n",
    "- 예를들어 주어진 데이터 $\\mathcal{D}= (x, t)$이 주택 가격에 관련된 데이터인데 여기서 $x$는 주택의 너비를 의미하고 $t$는 주택의 가격이라면 $x$가 크면 클 수록 $t$가 커지는 관계를 가질 것이라 예상 가능\n",
    "\n",
    "- 이 경우 $x^2$은 주택의 면적, $x^3$은 주택 공간의 부피로 주택에 대한 특성이라고 볼 수 있음.\n",
    "\n",
    "- 항상 물리적 의미를 부여할 수 있는 것은 아니며 이 경우도 $x^4$이상이라면 물리적 의미를 부여하기는 힘들어 진다.\n",
    "\n",
    "- 가설이 꼭 다항식일 필요는 없고, 실제로 다항식이 아닌 경우도 수업 마지막에 간단한 예제로 제시 되어 있다.\n",
    "\n",
    "- 다항식 가설을 만들기 위해 주어진 $x$에 다항 함수를 적용하였는데 이때 적용한 각각의 다항함수를 다음처럼 표기하기로 하자.\n",
    "\n",
    "$$\n",
    "\\phi_0(x) = x^0, \\, \\phi_{1}(x) = x^1, \\phi_{2}(x)=x^2, \\dots , \\phi_{M}(x) = x^M\n",
    "$$\n",
    "\n",
    "- 위 표기법으로 가설을 다시 적어보자.\n",
    "\n",
    "$$\n",
    "y(x, \\mathbf{w}) = w_0 \\phi_0(x) + w_1 \\phi_{1}(x) + w_2 \\phi_{2}(x) + \\cdots + w_M \\phi_{M}(x) = \\sum_{j=0}^{M} w_j \\phi_{j}(x)\n",
    "$$\n",
    "\n",
    "- 다시적은 $y(x,\\mathbf{w})$는 어떤 함수 $\\phi_{j}(x)$ (여기서는 이 어떤 함수가 다항함수인 경우에 대해 이야기하고 있지만 앞서 언급했듯이 어떤 함수가 되어도 상관없다.)에 $w_j$를 곱하여 다 더한 함수가 된다.\n",
    "\n",
    "- 따라서 $\\phi_{j}(x)$는 $y(x,\\mathbf{w})$를 만들기위한 재료가 되고 우리는 이것들을 $w_j$를 사용하여 잘 섞어서 결과로 나온 $y(x,\\mathbf{w})$를 실제 타겟과 최대한 비슷하게 만들어야 하는 상황이 된다. \n",
    "\n",
    "- 때문에 $\\phi_{j}(x)$를 기저함수<sup>basis function</sup>이라 한다.\n",
    "\n",
    "- 주어진 데이터가 $D$차원 벡터이고 이에 대한 기저함수 형식으로 가설을 가장 일반적인 형태로 표현하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "y(\\mathbf{x},\\mathbf{w}) = \n",
    "\\begin{bmatrix}\n",
    "w_0 & w_1 & w_2 & \\cdots & w_M\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}) \\\\[10pt]  \\phi_1(\\mathbf{x}) \\\\[10pt]  \\phi_2(\\mathbf{x}) \\\\[10pt]  \\vdots \\\\[10pt]  \\phi_M(\\mathbf{x})\n",
    "\\end{bmatrix} =\n",
    "\\mathbf{w}^{\\text{T}} \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "- 기저함수의 표현을 보면 벡터변수 $\\mathbf{x}$를 입력받아서 스칼라 출력을 내보내는 함수라는 것을 알 수 있다. \n",
    "\n",
    "- 더 나아가 모든 기저함수를 모아서 벡터 형식으로 쓰면 벡터 변수를 입력받아서 벡터를 출력하는 다변수 벡터 함수로 볼 수 도 있다.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\mathbf{x})=\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}) \\\\[10pt]  \\phi_1(\\mathbf{x}) \\\\[10pt]  \\phi_2(\\mathbf{x}) \\\\[10pt]  \\vdots \\\\[10pt]  \\phi_M(\\mathbf{x})\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 이제 여기서 우리가 결정해야할 사항은 \n",
    "    - 어떤 기저함수 형태를 사용할 것인가?\n",
    "    - 항을 몇개까지 설정할까? 즉 기저함수의 결정과 $M$을 얼마로 선택하는가 하는것으로 가설 또는 모델의 복잡도를 결정한다. 이 $M$에 대한 결정은 선형회귀 문제를 푸는 것과 상관없이 임의로 결정해야하는 값이며 하이퍼 파라미터라고 한다.\n",
    "    - $M+1$개의 $w$들은 어떻게 설정할까? 이 질문에 대한 답을 하는 것이 선형회귀 문제를 푸는 과정이 된다.\n",
    "    \n",
    "    \n",
    "- 현재로써는 기저함수의 형태는 다항식으로 결정했고 $M$과 그에 따른 $\\mathbf{w}$를 결정하는 구체적인 방법을 모르기 때문에 그냥 마음대로 결정하기로 하자.\n",
    "\n",
    "- 편한대로 다음처럼 결정한다. $M=3$, $w_0 = -2$, $w_1 = 1.5$, $w_2 = 0.3$, $w_3 = 3$\n",
    "\n",
    "$$\n",
    "y(x, \\mathbf{w}) = -2 + 1.5x + 0.3x^2 + 3 x^3\n",
    "$$\n",
    "\n",
    "- 아래 코드 결과를 보면 적당히 곡선성이 추가되었지만 여전히 잘 맞지 않고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = -2 + 1.5*x + 0.3*x**2 + 3*x**3\n",
    "y_pred = -2 + 1.5*x_train + 0.3*x_train**2 + 3*x_train**3\n",
    "\n",
    "print(\"w : {}, {}, {}, {}\".format(-2, 1.5, 0.3, 3))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([x_train, x_train],[y_pred, t_train], '--', color=style_colors[5], lw=2)\n",
    "plt.plot(x, y, label=\"$y(x, \\mathbf{w})$\", lw=3)\n",
    "plt.plot(x_train, t_train, 'o',  markersize=7 , label=r'Train data')\n",
    "plt.plot(x_train, y_pred, 'o',  markersize=7, color=style_colors[0])\n",
    "plt.xlabel('$x$', fontsize=20)\n",
    "plt.ylabel('$t$', fontsize=20)\n",
    "plt.title('Regression with coef.{},{},{},{}'.format(-2, 1.5, 0.3, 3))\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### numpy, sklearn을 이용한 선형회귀\n",
    "\n",
    "- 구체적으로 직접 알고리즘을 만들어보기 전에 두 라이브러리로 선형회귀를 수행하여 결과를 비교 해보자.\n",
    "\n",
    "- `numpy`에 최소제곱을 수행하는 간단한 `polyfit`함수 제공하고 `sklearn`에도 `LinearRegression`을 제공한다.\n",
    "\n",
    "- 두 결과는 동일 : 수학적으로 같은 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# 본격적으로 회귀를 하기 전에 기존 라이브러리를 이용하여 \n",
    "# 결과를 만들고 우리 알고리즘의 결과와 비교해보자.\n",
    "######################################################\n",
    "# M차 다항식\n",
    "# w가 5차원 벡터, 각 4차 계수, 각 3차 계수, 2차 계수, 1차 계수, 0차 계수(상수) \n",
    "M = 4\n",
    "\n",
    "#########################################################################\n",
    "## 넘파이 라이브러리를 사용해서 간단하게 구할 수 있다.\n",
    "#########################################################################\n",
    "# 1차 다항식\n",
    "z_lin    = np.polyfit(x_train, t_train, 1) \n",
    "\n",
    "# M차 다항식\n",
    "z_nonlin = np.polyfit(x_train, t_train, M)\n",
    "\n",
    "y_lin    = np.poly1d(z_lin)\n",
    "y_nonlin = np.poly1d(z_nonlin)\n",
    "#########################################################################\n",
    "\n",
    "#########################################################################\n",
    "## sklearn 라이브러리를 사용해서 간단(?)하게 구할 수 있다.\n",
    "#########################################################################\n",
    "# 1차 다항식\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(x_train[:, np.newaxis], t_train)\n",
    "y_sk_lin = model.predict(x[:, np.newaxis])\n",
    "\n",
    "# M차 다항식\n",
    "poly_model = make_pipeline(PolynomialFeatures(M), LinearRegression())\n",
    "poly_model.fit(x_train[:, np.newaxis], t_train)\n",
    "y_sk_nonlin = poly_model.predict(x[:, np.newaxis])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "fig.set_size_inches((15,6))\n",
    "\n",
    "ax1.plot(x_train, t_train, 'o', color=style_colors[1], label=r\"data\")\n",
    "ax1.plot(x, y_lin(x), linewidth=3, alpha=0.8, color=style_colors[0], label=r\"by numpy, linear\")\n",
    "ax1.plot(x, y_nonlin(x), linewidth=3, alpha=0.8, color=style_colors[2], label=r\"by numpy, non-linear\")\n",
    "ax1.legend(fontsize=13)\n",
    "ax1.set_title('numpy linear regression')\n",
    "\n",
    "ax2.plot(x_train, t_train, 'o', color=style_colors[1], label=r\"data\")\n",
    "ax2.plot(x, y_sk_lin, linewidth=3, alpha=0.8, color=style_colors[0], label=r\"by sklearn, linear\")\n",
    "ax2.plot(x, y_sk_nonlin, linewidth=3, alpha=0.8, color=style_colors[2], label=r\"by sklearn, non-linear\")\n",
    "ax2.legend(fontsize=13)\n",
    "ax2.set_title('sklearn linear regression')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"numpy coef.   : {}, Bias:{:.4f}\".format(z_nonlin[:-1], z_nonlin[-1]))\n",
    "print(\"sklearn coef. : {}, Bias:{:.4f}\".format(poly_model.steps[-1][1].coef_[::-1][:-1], \n",
    "                                      poly_model.steps[-1][1].intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- 이제 배운 지식을 무기로 `numpy`, `sklearn` 알고리즘 직접 만들어보자!\n",
    "\n",
    "\n",
    "- 문제를 풀기 위해 적절한 오차함수를 구성하고 이를 최소화 시키는 $\\mathbf{w}$를 찾기 위해 다음과 같은 두가지 방법을 고려하기로 한다.\n",
    "    - #### 경사 하강법\n",
    "    - #### 정규 방정식 풀이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 경사하강을 이용한 선형회귀\n",
    "\n",
    "- 위처럼 마음대로 정하는 것이 문제해결에 도움을 주지는 않지만 문제 해결 방향을 잡을 수 있게 해준다.\n",
    "\n",
    "- 시간이 무한대로 주어진다면 적절히 $\\mathbf{w}$를 조정하면서 함수값이 최소 근처가 되도록 조정할 수 있다.\n",
    "\n",
    "- 우리도 linreg.html 파일을 통해 같은 작업을 해본적이 있지만 결정해야하는 $\\mathbf{w}$의 요소 개수가 많아지면 힘들어진다.\n",
    "\n",
    "- 문제를 제대로 해결하기 위해 다음과 같은 에러 함수를 정의\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} \\{ y(x_n, \\mathbf{w}) - t_n \\}^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2N} \\sum_{n=1}^{N} \\{ y(x_n, \\mathbf{w}) - t_n \\}^2 \n",
    "$$\n",
    "\n",
    "\n",
    "- $x_n$, $t_n$는 변수가 아니고 우리가 가지고 있는 점 데이터\n",
    "\n",
    "- 여기서 변수는 계수 $w_0, w_1, \\dots, w_M $, $M+1$개\n",
    "\n",
    "- 우리의 가설이 타겟과 최대한 비슷하기를 바라는데 가설과 타겟의 차이는 $ y(x_n, \\mathbf{w}) - t_n$ 부분이다.\n",
    "\n",
    "- 따라서 위 함수는 우리의 가설과 타겟값의 차이를 제곱해서 모두 더한 값을 계산해주는 함수이다.\n",
    "\n",
    "- 조금전 우리 마음대로 정한 계수값에 대해서 에러 값 계산해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# 우리 마음대로 정한 변수w를 이용하여 오차를 계산\n",
    "######################################################\n",
    "y = -2 + 1.5*x_train + 0.3*x_train**2 + 3*x_train**3\n",
    "\n",
    "print(0.5*(((y - t_train)**2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 꽤 큰값이 계산되는데 이 값을 최대한 0으로 만드는 것이 우리의 목적이 된다.\n",
    "\n",
    "- 위처럼 오차의 제곱을 최소화시키기를 원하기 때문에 최소제곱법<sup>least square</sup>이라는 이름으로 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 경사하강법\n",
    "\n",
    "- 위처럼 에러 함수가 정의되었으면 최적화 수법을 에러 함수 $E(\\mathbf{w})$에 적용해서 문제를 해결할 수 있다.\n",
    "\n",
    "- 즉, 정의한 에러함수가 최적화의 목적함수가 되고 목적함수를 정의한 이후의 과정은 최적화 과정과 동일하다.\n",
    "\n",
    "#### 에러 함수\n",
    "\n",
    "- 다음과 같은 에러 함수를 계산하는 코드\n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} \\{ y(x_n, \\mathbf{w}) - t_n \\}^2 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def E(w, M, x, t):\n",
    "    \"\"\"\n",
    "    Error function\n",
    "    E(w)= (1/2) * sum_{n=1}^{N} {y(x_n,w) - t_n}^2\n",
    "    y(x_n, w) = w_0*x^0 + w_1*x^1 + w_2*x^2 + ... + w_M*x^M\n",
    "    \"\"\"\n",
    "    X = np.array([ x**i for i in range(M+1) ])  \n",
    "    y = (w.reshape(-1,1) * X).sum(axis=0)\n",
    "    \n",
    "    return 0.5*(( (y - t)**2 ).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)\n",
    "X = np.array([ x_train**i for i in range(M+1) ])  \n",
    "print(X)\n",
    "\n",
    "w = np.random.rand(M+1)\n",
    "print(w)\n",
    "print((w.reshape(-1,1) * X).sum(axis=0))\n",
    "\n",
    "# sum 없이 하려면?\n",
    "print(np.dot(X.T, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 에러함수의 경사도벡터\n",
    "\n",
    "- 수치미분 코드를 사용 하거나 또는 직접 미분하여 다음처럼 함수를 만들어 사용할 수도 있음\n",
    "\n",
    "- 자세한 미분 과정은  정규방정식 유도에서 다시 이야기 함\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E(\\mathbf{w})}{\\partial \\, \\mathbf{w}} \n",
    "&=   \\sum_{n=1}^{N} \\left( \\mathbf{w}^{\\text{T}} \\boldsymbol{\\phi}(\\mathbf{x}_n) \\,   - t_n \\right) \\boldsymbol{\\phi}(\\mathbf{x}_n)^{\\text{T}} \\\\[15pt]\n",
    "&= \\mathbf{w}^{\\text{T}} \\left( \\sum_{n=1}^{N}  \\boldsymbol{\\phi}(\\mathbf{x}_n) \\, \\boldsymbol{\\phi}(\\mathbf{x}_n)^{\\text{T}} \\right) - \\sum_{n=1}^{N}  t_n \\boldsymbol{\\phi}(\\mathbf{x}_n)^{\\text{T}} \\\\[15pt]\n",
    "&= \\mathbf{w}^{\\text{T}} \\left( \\mathbf{\\Phi}^{\\text{T}} \\mathbf{\\Phi} \\right) -\\boldsymbol{\\mathsf{t}}^{\\text{T}} \\mathbf{\\Phi}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- 위 식은 스칼라를 벡터로 미분할 때 분자 레이아웃을 써서 결과 벡터가 행벡터임을 유의해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_anal(w, M, x, t):\n",
    "    \"\"\"\n",
    "    This function computes the analytic gradient of the objective function\n",
    "    x, t : data for error function eval.\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    PI = np.hstack( np.array( [np.power(x.reshape(N,1), p) for p in range(M+1)] ) )\n",
    "    \n",
    "    # 위 식을 그대로 코딩\n",
    "    # g_  = np.dot(w.T, np.dot(PI.T, PI) ) - np.dot(t.T, PI)\n",
    "    \n",
    "    # 식을 정리해서 np.dot 2번으로...\n",
    "    g = np.dot( np.dot(w.T,PI.T)-t.T , PI)\n",
    "    \n",
    "    # print(g)\n",
    "    # print(g.shape)\n",
    "    # print(g_)\n",
    "    # print(g_.shape)\n",
    "    \n",
    "    return g.astype('longdouble') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# 혹시 아래쪽에서 E함수를 레귤러라이제이션 항이 있는 버전으로 덮어썼을 가능성이 있기 때문에\n",
    "# 여기서 다시 레귤러라이제이션 항을 없애준다.\n",
    "regularizer = ''\n",
    "\n",
    "# M을 0, 1, 3, 8에 대해서 한번씩 피팅\n",
    "ms = [0, 1, 3, 8]\n",
    "\n",
    "# 피팅결과 구해진 파라미터를 W에 저장\n",
    "W = []\n",
    "\n",
    "# 학습데이터를 넘기면서 fitting\n",
    "for M in ms :\n",
    "    x0 = np.random.uniform(-1, 1, M+1)\n",
    "    \n",
    "    ############################################################################\n",
    "    # 우리가 만든 minimize함수를 사용하는 경우\n",
    "#     W.append(minimize(E, x0, args=(M, x_train, t_train), \n",
    "#                       jac=grad_anal, method=\"CGFR\", verbose_step=500))   \n",
    "    ############################################################################\n",
    "    \n",
    "    ############################################################################\n",
    "    # scipy.optimize.minimize 사용하는 경우\n",
    "    ret = optimize.minimize(E, x0,  args=(M, x_train, t_train), \n",
    "                            jac=grad_anal, method='BFGS', \n",
    "                            options={'gtol': 1e-08,})\n",
    "    W.append(ret.x)\n",
    "    print(ret)\n",
    "    ############################################################################\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex='all', sharey='all')\n",
    "fig.set_size_inches((16,13))\n",
    "\n",
    "j = 0\n",
    "ax[0,0].set_ylim(-2,2)\n",
    "\n",
    "for M in ms :\n",
    "    p, q = divmod(j, 2)\n",
    "    X = np.array([ x**i for i in range(M+1) ])  \n",
    "    y = (W[j].reshape(-1,1) * X).sum(axis=0)\n",
    "\n",
    "    ax[p,q].plot(x, t, '--', lw=3, alpha=0.5, label=\"True\")\n",
    "    ax[p,q].plot(x_train, t_train, 'o', alpha=0.8)\n",
    "    ax[p,q].plot(x, y, lw=3, label=\"$M=${}\".format(M))\n",
    "    ax[p,q].legend(loc='upper right', fontsize=20)\n",
    "\n",
    "    j+=1\n",
    "\n",
    "plt.suptitle('Plots of polynomials having various orders $M$', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# 회귀계수를 모아서 테이블 형태로 출력\n",
    "A = np.full((9, 4), np.nan)\n",
    "for i in range(len(W)) :\n",
    "    A[0:W[i].shape[0], i] = W[i]\n",
    "\n",
    "df = pd.DataFrame(data=A,  columns=['M=0', 'M=1', 'M=3', 'M=8'])\n",
    "display(df)  # OR print(df.to_html()) or display(HTML(df.to_html()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 평가하기\n",
    "\n",
    "$$\n",
    "E_{\\text{RMS}} = \\sqrt{\\frac{2 E(\\mathbf{w}^{*})}{N}}\n",
    "$$\n",
    "\n",
    "- 오차함수값에 루트를 씌워 데이터와 동일한 스케일을 가지도록 하고(오차함수에 이미 제곱이 되어있기 때문)\n",
    "\n",
    "- $N$으로 나누어 평균적인 오차를 알아볼 수 있게 함.\n",
    "\n",
    "- 2 곱하기는 오차함수에 (1/2)이 곱해져 있기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def rms(w, E, M, x, t) :\n",
    "    return np.sqrt( (2*E(w, M, x, t)) / x.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 에러 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# M을 0~(M_overfit-1)에 대해서 한번씩 피팅\n",
    "M_overfit = 9\n",
    "Ms = np.arange(M_overfit)\n",
    "\n",
    "# 피팅결과 구해진 파라미터를 W에 저장\n",
    "W = []\n",
    "\n",
    "# 학습데이터를 넘기면서 fitting\n",
    "for M in Ms :\n",
    "    x0 = np.random.uniform(-1, 1, M+1)\n",
    "    \n",
    "    ############################################################################\n",
    "    # 우리가 만든 minimize함수를 사용하는 경우\n",
    "    # W.append(minimize(E, x0, args=(M, x_train, t_train), \n",
    "    #                   jac=grad_anal, method=\"CGFR\", verbose_step=500))  \n",
    "    ############################################################################\n",
    "    \n",
    "    ############################################################################\n",
    "    # scipy.optimize.minimize 사용하는 경우\n",
    "    ret = optimize.minimize(E, x0,  args=(M, x_train, t_train), \n",
    "                            jac=grad_anal, method='BFGS', \n",
    "                            options={'gtol': 1e-08,})\n",
    "    print(ret)\n",
    "    W.append(ret.x)\n",
    "    ############################################################################\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 회귀계수를 모아서 테이블 형태로 출력\n",
    "A = np.full((len(Ms), len(Ms)), np.nan)\n",
    "for i in range(len(W)) :\n",
    "    A[0:W[i].shape[0], i] = W[i]\n",
    "    \n",
    "df = pd.DataFrame(data=A,  columns=['M={}'.format(i) for i in range(len(Ms))])\n",
    "display(df)  # OR print(df.to_html()) or display(HTML(df.to_html()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차에서 9차까지 회귀 결과에 대해 학습, 테스트 데이터로 각각 RMS를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMS = np.zeros((len(Ms), 3))\n",
    "\n",
    "for i in Ms :\n",
    "    RMS[i, 0] = i\n",
    "    RMS[i, 1] =rms(W[i], E, W[i].shape[0]-1, x_train, t_train)\n",
    "    \n",
    "for i in Ms :\n",
    "    RMS[i, 2] =  rms(W[i], E, W[i].shape[0]-1, x_test, t_test)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(RMS[:,0], RMS[:,1], 'o-', lw=3, markersize=13, fillstyle='none',  label='train')    \n",
    "plt.plot(RMS[:,0], RMS[:,2], 'o-', lw=3, markersize=13, fillstyle='none', label='test')\n",
    "plt.title('Overfitting')\n",
    "plt.xticks( np.arange(13))\n",
    "\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('RMS')\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 과적합<sup>Over-ftting</sup>\n",
    "\n",
    "- 훈련 데이터에 잘 맞으나 그외의 데이터에 잘 맞지 않는 상태\n",
    "\n",
    "- 위 그래프에서 다항식의 차수가 7차를 넘어가는 순간 과적합 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 더 많은 데이터를 모아서 해결\n",
    "\n",
    "- 더 많은 데이터가 문제를 해결 할 수 있는지 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 8차로 더 많은 데이터를 모아 선형회귀 시도\n",
    "###########################################\n",
    "M = 8\n",
    "\n",
    "x0 = np.random.uniform(-1, 1, M+1)\n",
    "\n",
    "############################################################################\n",
    "# 우리가 만든 minimize함수를 사용하는 경우\n",
    "# w = minimize(E, x0, args=(M, x_test, t_test), \n",
    "#              jac=grad_anal, method=\"CGFR\", verbose_step=500)\n",
    "############################################################################\n",
    "\n",
    "############################################################################\n",
    "# scipy.optimize.minimize 사용하는 경우\n",
    "ret = optimize.minimize(E, x0,  args=(M, x_test, t_test), \n",
    "                            jac=grad_anal, method='BFGS', \n",
    "                            options={'gtol': 1e-08,})\n",
    "print(ret)\n",
    "w = ret.x\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 많은 데이터에 대한 회귀결과 출력\n",
    "###########################################\n",
    "X = np.array([ x**i for i in range(M+1) ])  \n",
    "y = (w.reshape(-1,1) * X).sum(axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, t, '--', lw=3, alpha=0.5)\n",
    "plt.plot(x_test, t_test, 'o', alpha=0.8)\n",
    "plt.plot(x, y, lw=3)\n",
    "plt.title(\"Result from regression with test data set\")\n",
    "plt.show()\n",
    "\n",
    "print(\"W:{}\".format(w.reshape(1,-1)[::-1][0].astype('float64')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 레귤러라이제이션<sup>regularization</sup>를 통한 해결\n",
    "\n",
    "- 현실적으로 데이터를 많이 모을 수 없음\n",
    "\n",
    "- 우선 다음과 같은 항을 에러함수에 추가\n",
    "\n",
    "$$\n",
    "\\tilde{E}(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N} \\left\\{ y(x_n, \\mathbf{w}) - t_n \\right\\}^2 + \\color{OrangeRed}{\\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert^2}\n",
    "$$\n",
    "\n",
    "- 직관적 해석 : 에러함수에 $\\mathbf{w}$의 요소가 제곱되어 다 더해진 항이 추가 되면서 $w_j$가 커지면 에러값이 커지게 설계\n",
    "\n",
    "- 절편항에 대한 회귀 계수 $w_0$는 레귤러라이제이션에서 제외하는 경우도 있음(sklearn은 이런식으로 구현됨)\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# L2 Reg. term을 가진 에러함수를 다시 정의하고 피팅 시도\n",
    "###########################################\n",
    "def E_L2(w, M, x, t):\n",
    "    \"\"\"\n",
    "    Error function with L2 Reg. term\n",
    "    \"\"\"\n",
    "    X = np.array([ x**i for i in range(M+1) ])  \n",
    "    y = (w.reshape(-1,1) * X).sum(axis=0)\n",
    "    \n",
    "    #                                                L2 reg. term\n",
    "    return 0.5*(( (y - t)**2 ).sum()) + (lamda/2.) * np.linalg.norm(w[1:])**2\n",
    "\n",
    "M = 8\n",
    "lamda = np.exp(-10)\n",
    "x0 = np.random.uniform(-1, 1, M+1)\n",
    "\n",
    "############################################################################\n",
    "# 우리가 만든 minimize함수를 사용하는 경우\n",
    "# w = minimize(E_L2, x0, args=(M, x_test, t_test, lamda), \n",
    "#              method=\"CGFR\", verbose_step=500)\n",
    "############################################################################\n",
    "\n",
    "############################################################################\n",
    "# scipy.optimize.minimize 사용하는 경우\n",
    "ret = optimize.minimize(E_L2, x0,  args=(M, x_train, t_train), method='BFGS', )\n",
    "print(ret)\n",
    "w = ret.x\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# L2 레귤러라이제이션 결과 출력\n",
    "###########################################\n",
    "X = np.array([ x**i for i in range(M+1) ])  \n",
    "y = (w.reshape(-1,1) * X).sum(axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, t, '--', lw=3, alpha=0.5)\n",
    "plt.plot(x_train, t_train, 'o', alpha=0.8)\n",
    "plt.plot(x, y, lw=3)\n",
    "plt.title(\"Result from regression with train data set and L2 Reg.\")\n",
    "plt.show()\n",
    "\n",
    "print(\"W:{}\".format(w.reshape(1,-1)[::-1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 레귤러라이제이션 사용 결과 : 많은 데이터를 모으지 않고도 9차 다항식으로 괜찮은 결과 생성\n",
    "\n",
    "- 주로 사용되는 레귤러라이제이션 : L1, L2 레귤러라이제이션\n",
    "\n",
    "- 아래는 L1, L2 노름의 형상화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "w = np.linspace(-10,10,100)\n",
    "W1, W2 = np.meshgrid(w, w)\n",
    "\n",
    "#L2\n",
    "L2 = W1**2 + W2**2\n",
    "\n",
    "#L1 \n",
    "L1 = np.abs(W1) + np.abs(W2)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches((13,13))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "ax4 = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "\n",
    "CS = ax1.contour(W1, W2, L1)\n",
    "ax1.clabel(CS, CS.levels,  fmt='%1.0f', inline=True, fontsize=10)\n",
    "ax1.axis('equal')\n",
    "ax1.set_title(r'$||x||_{1}$ norm')\n",
    "\n",
    "CS = ax2.contour(W1, W2, L2)\n",
    "ax2.clabel(CS, CS.levels,  fmt='%1.0f', inline=True, fontsize=10)\n",
    "ax2.axis('equal')\n",
    "ax2.set_title(r'$||x||_{2}$ norm')\n",
    "\n",
    "ax3.plot_surface(W1, W2, L1, rstride=5, cstride=5, alpha=0.3)\n",
    "cset = ax3.contour(W1, W2, L1, zdir='z', offset=0, cmap='viridis')\n",
    "ax3.set_xlabel(r'$x_1$')\n",
    "ax3.set_ylabel(r'$x_2$')\n",
    "ax3.set_zlabel(r'$||x||_{1}$')\n",
    "ax3.set_title(r'$||x||_{1}$ norm')\n",
    "\n",
    "ax4.plot_surface(W1, W2, L2, rstride=5, cstride=5, alpha=0.3)\n",
    "cset = ax4.contour(W1, W2, L2, zdir='z', offset=0, cmap='viridis')\n",
    "ax4.set_xlabel(r'$x_1$')\n",
    "ax4.set_ylabel(r'$x_2$')\n",
    "ax4.set_zlabel(r'$||x||_{2}$')\n",
    "ax4.set_title(r'$||x||_{2}$ norm')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### L1, L2 레귤러라이제이션 실험\n",
    "\n",
    "- 아래는 L1, L2 레귤러라이제이션의 실험코드\n",
    "\n",
    "- 에러 함수안에 lamda를 변화시켜가면서 제약조건과 최적해거 어떻게 결정되는지 확인\n",
    "\n",
    "- L1 레귤러라이제이션항이 추가된 목적함수는 미분 불가능한 점이 생김\n",
    "\n",
    "- L1 레귤러라이제이션의 경우 $w_j=0$인 지점에서 미분계수 0을 사용\n",
    "\n",
    "- 이를 위해 L1항이 추가된 목적함수는 수치 미분하지 않고 직접 미분한 함수를 사용<sup>[3],[4],[5]</sup> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# L1, L2 Reg.의 효과를 비교하기 위한 프로그램\n",
    "#\n",
    "\n",
    "def E_reg(w, M, x, t):\n",
    "    \"\"\"\n",
    "    Error function with L1 or L2 Reg. term\n",
    "    Calculate differently according to global variable; regularizer and reg_intercept\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    X = np.array([ x**i for i in range(M+1) ])  \n",
    "    y = (w.reshape(-1,1) * X).sum(axis=0)\n",
    "    \n",
    "    if regularizer == 'L2' :\n",
    "        # L2 reg.\n",
    "        if not reg_intercept :\n",
    "            return 0.5*(( (y - t)**2 ).sum()) + (lamda/2.)*np.linalg.norm(w[1:])**2 \n",
    "        else :\n",
    "            return 0.5*(( (y - t)**2 ).sum()) + (lamda/2.)*np.linalg.norm(w)**2   \n",
    "    elif regularizer == 'L1' :\n",
    "        # L1 reg.\n",
    "        if not reg_intercept :\n",
    "            return 0.5*(( (y - t)**2 ).sum()) + (lamda/2.)*(np.abs(w[1:]).sum()) \n",
    "        else :\n",
    "            return 0.5*(( (y - t)**2 ).sum()) + (lamda/2.)*(np.abs(w).sum())   \n",
    "    else :\n",
    "        return 0.5*(( (y - t)**2 ).sum())\n",
    "       \n",
    "def grad_L1(w, M, x, t):\n",
    "    \"\"\"\n",
    "    This function computes the analytic gradient of the objective function with L1 reg. term\n",
    "    x, t : data for error function eval.\n",
    "    \"\"\"\n",
    "    # 다항함수를 기반으로하는 design matrix Φ를 만든다.\n",
    "    PI = np.hstack( np.array([np.power(x.reshape(N,1), p) for p in range(M+1)]) )\n",
    "    g = np.dot(w.T, np.dot(PI.T, PI) ) - np.dot(t.T, PI)\n",
    "    \n",
    "    if not reg_intercept :\n",
    "        g_reg = np.hstack( np.array([0 , (lamda/2.)*np.sign(w[1:])]) ) \n",
    "    else :\n",
    "        g_reg = (lamda/2.)*np.sign(w)       \n",
    "        \n",
    "    g += g_reg\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# 전역변수 설정\n",
    "\n",
    "# 직선으로 회귀, 여기서는 바꾸면 안됨, w의 그림을 그리기 위해. 2개 이상되면 그림 못 그림\n",
    "M = 1          \n",
    "\n",
    "# 레귤러라이제이션 상수이자 동시에 제약조건에 할당된 라그랑지승수\n",
    "lamda = None         \n",
    "\n",
    "# L1 or L2 레귤러라이제이션 선택, L1, L2바꿔가면서 실험해볼 수 있음\n",
    "regularizer = 'L2' \n",
    "\n",
    "# 레귤러라이제이션에서 절편항을 포함시킬지 말지를 결정\n",
    "reg_intercept = True\n",
    "################################################################\n",
    "\n",
    "# 여기서 람다를 바꿔가면서 여러번 fit\n",
    "if regularizer == 'L1' :\n",
    "    lambdas = np.linspace(0, 3, 30)\n",
    "else :\n",
    "    lambdas = np.linspace(0, 15, 40)\n",
    "    \n",
    "ws = np.zeros((len(lambdas), M+1))\n",
    "x0 = np.random.uniform(-1, 1, M+1)\n",
    "\n",
    "for i, lamda in enumerate(lambdas) :\n",
    "    if regularizer == 'L1' :\n",
    "        ############################################################################\n",
    "        # 우리가 만든 minimize함수를 사용하는 경우\n",
    "        # w = minimize(E_reg, x0, args=(M, x_train, t_train), \n",
    "        #              jac=grad_L1, strict=False, max_iter=1500, verbose=False)\n",
    "        ############################################################################\n",
    "        \n",
    "        ############################################################################\n",
    "        # scipy.optimize.minimize 사용하는 경우\n",
    "        ret = optimize.minimize(E_reg, x0,  args=(M, x_train, t_train), \n",
    "                                jac=grad_L1, method='SLSQP',)\n",
    "        w = ret.x\n",
    "        ############################################################################\n",
    "        \n",
    "    else :\n",
    "        ############################################################################\n",
    "        # 우리가 만든 minimize함수를 사용하는 경우\n",
    "        # w = minimize(E, x0, args=(M, x_train, t_train), max_iter=1500, verbose=False)\n",
    "        ############################################################################\n",
    "                \n",
    "        ############################################################################\n",
    "        # scipy.optimize.minimize 사용하는 경우\n",
    "        ret = optimize.minimize(E_reg, x0,  args=(M, x_train, t_train), \n",
    "                                method='SLSQP', )\n",
    "        w = ret.x\n",
    "        ############################################################################\n",
    "        \n",
    "    ws[i,:] = w\n",
    "    print(\"lambda:{:f}, w:{}\".format(lamda, w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "# 2개의 파라미터로 회귀한 결과를 출력\n",
    "# 파라미터가 2개라서 에러함수는 2변수 실함수가 되고 등고선 플롯으로 그릴 수 있다.\n",
    "# 비제약 최적해와 제약조건산에 결정된 최적해 비교\n",
    "######################################################\n",
    "\n",
    "# 회귀계수 공간에서 메쉬그리드 만들기\n",
    "ww = np.linspace(-2, 2, 200)\n",
    "W1, W2 = np.meshgrid(ww, ww)\n",
    "W = np.array([W1, W2])\n",
    "\n",
    "# 앞선 9차까지 순차적으로 계샌해둔 결과중에서 직선회귀 결과를 이용해서\n",
    "# 무제약 최적해를 초기화\n",
    "w_unconst =  np.array(A[0:2,1])\n",
    "\n",
    "#############################################################\n",
    "# 제약조건 함수\n",
    "L2 = W1**2 + W2**2 #L2\n",
    "L1 = np.abs(W1) + np.abs(W2) #L1 \n",
    "L = {'L1':L1, 'L2':L2}\n",
    "#############################################################\n",
    "\n",
    "#############################################################\n",
    "# 제약조건에 나타나는 eta 계산 eq(1.4)\n",
    "eta_L1 = np.abs(ws[0,:]).sum()\n",
    "eta_L2 = (ws[0,:]**2).sum()\n",
    "eta = {'L1':eta_L1, 'L2':eta_L2}\n",
    "#############################################################\n",
    "\n",
    "#############################################################\n",
    "# 에러함수의 제약조건 있는 상태와 없는 상태를 계산\n",
    "# 루프 돌리면 구현은 직관적이나 비효율적\n",
    "E_unconst = np.zeros_like(W1)\n",
    "E_const = np.zeros_like(W1)\n",
    "\n",
    "for i in range(W1.shape[0]) :\n",
    "    for j in range(W1.shape[1]) :\n",
    "        E_unconst[i,j] = E(W[:,i,j], M, x_train, t_train) \n",
    "        E_const[i,j]   = E_reg(W[:,i,j], M, x_train, t_train) \n",
    "#############################################################\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches((14,7))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "# 여기서 바로 애니메이션으로 만들기\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# ax1\n",
    "# 무제약 에러함수 그림\n",
    "CS1 = ax1.contour(W1, W2, E_unconst,  levels=[2, 3, 4, 8, 16, 24, 32, 40, 48])\n",
    "ax1.clabel(CS1, CS1.levels,  fmt='%.3f', inline=True, fontsize=10)\n",
    "\n",
    "# 제약 최적해와 무제약 최적해를 그림\n",
    "# 유용 영역 그림\n",
    "CS1_cst = ax1.contourf(W1, W2, L[regularizer], levels=[0, eta[regularizer]], alpha=0.4)\n",
    "sol1, = ax1.plot(ws[:,0], ws[:,1], '.', color=style_colors[7], label='Solution')\n",
    "ax1.plot(*w_unconst, 'o', color=style_colors[1])\n",
    "ax1.set_xlabel('$W_0$', fontsize=15)\n",
    "ax1.set_ylabel('$W_1$', fontsize=15)\n",
    "ax1.legend(fontsize=15)\n",
    "ax1.set_title('Unconstrained error func. and its feasible region')\n",
    "ax1.axis('equal')\n",
    "\n",
    "##################################################################################\n",
    "# ax2\n",
    "# 제약 에러함수 그림\n",
    "CS2 = ax2.contour(W1, W2, E_const, levels=[2, 3, 4, 8, 16, 24, 32, 40, 48])\n",
    "ax2.clabel(CS2, CS2.levels,  fmt='%.3f', inline=True, fontsize=10)\n",
    "\n",
    "# 제약 최적해\n",
    "sol2, = ax2.plot(ws[:,0], ws[:,1], '.', color=style_colors[7], label='Solution')\n",
    "ax2.set_xlabel('$W_0$', fontsize=15)\n",
    "ax2.set_ylabel('$W_1$', fontsize=15)\n",
    "ax2.set_title('Constrained error func.')\n",
    "ax2.axis('equal');\n",
    "\n",
    "\n",
    "def update(i):\n",
    "    global CS1_cst, CS2, lamda\n",
    "    \n",
    "    # 유용역역 업데이트\n",
    "    #############################################################\n",
    "    # 제약조건에 나타나는 eta 계산 eq(1.4)\n",
    "    eta_L1 = np.abs(ws[i,:]).sum()\n",
    "    eta_L2 = (ws[i,:]**2).sum()\n",
    "    eta = {'L1':eta_L1, 'L2':eta_L2}\n",
    "    \n",
    "    for c in CS1_cst.collections:\n",
    "        c.remove()  # removes only the contours, leaves the rest intact\n",
    "    CS1_cst = ax1.contourf(W1, W2, L[regularizer], levels=[0, eta[regularizer]], alpha=0.4)\n",
    "    \n",
    "    #############################################################\n",
    "    # 제약조건 추가된 에러함수 업데이트\n",
    "    lamda = lambdas[i]\n",
    "    for j in range(W1.shape[0]) :\n",
    "        for k in range(W1.shape[1]) :\n",
    "            E_const[j,k]   = E(W[:,j,k], M, x_train, t_train) \n",
    "            \n",
    "    for c in CS2.collections:\n",
    "        c.remove()  # removes only the contours, leaves the rest intact\n",
    "    CS2 = ax2.contour(W1, W2, E_const, levels=[2, 3, 4, 8, 16, 24, 32, 40, 48])\n",
    "\n",
    "    #솔루션 업데이트\n",
    "    sol1.set_data(ws[i,0], ws[i,1])\n",
    "    sol2.set_data(ws[i,0], ws[i,1])\n",
    "\n",
    "    \n",
    "# 이 부분은 시간이 많이 걸리기 때문에 필요할때만 주석을 제거하여 실행해야함    \n",
    "# ani = animation.FuncAnimation(fig, update, frames=np.arange(ws.shape[0]), interval=100)\n",
    "# rc('animation', html='html5')\n",
    "# HTML(ani.to_html5_video())\n",
    "\n",
    "# gif로 출력하려면 다음 라인 사용한다. imagemagick 라이브러리가 설치되어 있어야 함.\n",
    "# ani.save('L2.gif', dpi=80, writer='imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "ax1.plot_surface(W1, W2, E_unconst, rstride=10, color=style_colors[5],  cstride=10, alpha=1)\n",
    "ax1.view_init(15, 30)\n",
    "ax1.set_xlabel('$W_1$')\n",
    "ax1.set_ylabel('$W_2$')\n",
    "ax1.set_title(\"Leaset Square\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "ax2.plot_surface(W1, W2, E_const, rstride=10,  color=style_colors[6], cstride=10,  alpha=1)\n",
    "ax2.view_init(15, 30)\n",
    "ax2.set_xlabel('$W_1$')\n",
    "ax2.set_ylabel('$W_2$')\n",
    "\n",
    "if regularizer == 'L1' :\n",
    "    ax2.set_title(\"Lasso\")\n",
    "else:\n",
    "    ax2.set_title(\"Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://plot.ly/python/\n",
    "# https://plot.ly/python/offline/\n",
    "\n",
    "# hover contour off\n",
    "# https://community.plot.ly/t/trace-lines-appearing-on-hover/5801\n",
    "data = [\n",
    "    go.Surface(x=W1, y=W2, z=E_const, showscale=False, colorscale='Portland',\n",
    "               contours=dict(\n",
    "                   x=dict(show=False,  highlight=False),\n",
    "                   y=dict(show=False,  highlight=False),\n",
    "                   z=dict(show=True,  highlight=True, highlightcolor=\"#A60628\"),\n",
    "               ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Regularized Objective Function',\n",
    "    autosize=False, \n",
    "    width=600, height=600,\n",
    "    margin=dict(l=0, r=0, b=0, t=30)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "1. [Bishop] Pattern Recognition and Machine Learning, Christopher Bishop, Springer\n",
    "\n",
    "3. [why-ridge] https://stats.stackexchange.com/questions/118712/why-does-ridge-estimate-become-better-than-ols-by-adding-a-constant-to-the-diago/119708#119708\n",
    "\n",
    "4. [오일석] 기계학습, 오일석, 한빛미디어\n",
    "\n",
    "5. [Geron] Hands-On Machine Learning(핸즈온 머신러닝), Aurelien Geron(박해선 역), O'Reilly(한빛미디어)\n",
    "\n",
    "6. [subgradient-wiki] https://en.wikipedia.org/wiki/Subgradient_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def _set_css_style(css_file_path):\n",
    "   \"\"\"\n",
    "   Read the custom CSS file and load it into Jupyter.\n",
    "   Pass the file path to the CSS file.\n",
    "   \"\"\"\n",
    "   styles = open(css_file_path, \"r\").read()\n",
    "   s = '<style>%s</style>' % styles     \n",
    "   return HTML(s)\n",
    "\n",
    "_set_css_style(\"../../style.css\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
